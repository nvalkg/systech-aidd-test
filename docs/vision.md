# Vision - Техническое видение проекта LLM-ассистента

## Технологии

### Основные технологии
- **Python 3.11+** - основной язык разработки
- **uv** - быстрый менеджер зависимостей (замена pip/poetry)
- **aiogram 3.x** - современная асинхронная библиотека для Telegram Bot API с поддержкой polling
- **openai** - официальный клиент для работы с LLM через OpenRouter
- **python-dotenv** - загрузка переменных окружения
- **make** - автоматизация сборки и запуска

### Обоснование выбора
- **uv** - в 10-100 раз быстрее pip, простая настройка
- **aiogram** - самая популярная и стабильная библиотека для Telegram ботов в Python
- **polling** - проще webhook'ов для MVP, не требует публичного URL
- **OpenRouter** - единый API для доступа к разным LLM моделям

### Структура зависимостей
```
pyproject.toml - конфигурация проекта и зависимости
requirements.txt - для совместимости
Makefile - команды сборки
.env.example - шаблон переменных окружения
```

## Принцип разработки

### Основные принципы
- **KISS (Keep It Simple, Stupid)** - максимальная простота, никакого оверинжиниринга
- **ООП с жестким соблюдением правила "1 класс = 1 файл"**
- **Минимализм** - только необходимый функционал для проверки идеи
- **Асинхронность** - использование async/await для эффективной работы с API

### Структура классов
Каждый класс в отдельном файле с четкой ответственностью:

1. **`TelegramBot`** (`telegram_bot.py`) - только работа с Telegram API
2. **`LLMClient`** (`llm_client.py`) - только взаимодействие с OpenRouter
3. **`ConversationManager`** (`conversation_manager.py`) - только управление диалогом
4. **`Config`** (`config.py`) - только загрузка конфигурации

### Принципы кодирования
- Один класс = одна ответственность
- Минимум зависимостей между классами
- Простые методы без сложной логики
- Прямолинейный код без абстракций
- Обработка ошибок на верхнем уровне

### Структура файлов
```
src/
├── telegram_bot.py
├── llm_client.py
├── conversation_manager.py
├── config.py
└── main.py
```

## Структура проекта

### Минимальная файловая структура
```
telegram-llm-bot/
├── src/
│   ├── telegram_bot.py      # TelegramBot класс
│   ├── llm_client.py        # LLMClient класс  
│   ├── conversation_manager.py  # ConversationManager класс
│   ├── config.py            # Config класс
│   └── main.py              # Точка входа
├── pyproject.toml           # Конфигурация проекта и зависимости
├── requirements.txt         # Для совместимости
├── Makefile                 # Команды сборки
├── .env.example             # Шаблон переменных окружения
├── .env                     # Локальные настройки (в .gitignore)
├── .gitignore
└── README.md
```

### Описание файлов
- **`src/`** - весь исходный код
- **`pyproject.toml`** - конфигурация uv, зависимости, метаданные
- **`requirements.txt`** - для совместимости с другими инструментами
- **`Makefile`** - простые команды: `make install`, `make run`, `make dev`
- **`.env.example`** - шаблон с необходимыми переменными
- **`.env`** - реальные настройки (не коммитится)

### Команды Makefile
```makefile
install:     # uv sync
run:         # python src/main.py  
dev:         # uv run python src/main.py
clean:       # очистка кэша
```

## Архитектура проекта

### Основные компоненты и их взаимодействие

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   TelegramBot   │───▶│ConversationManager│───▶│   LLMClient     │
│                 │    │                  │    │                 │
│ - Получение     │    │ - Управление     │    │ - Запросы к     │
│   сообщений     │    │   диалогом       │    │   OpenRouter    │
│ - Отправка      │    │ - Формирование   │    │ - Обработка     │
│   ответов       │    │   промптов       │    │   ответов       │
└─────────────────┘    └──────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌──────────────────┐
                    │     Config       │
                    │                  │
                    │ - Токены         │
                    │ - Настройки      │
                    └──────────────────┘
```

### Поток данных
1. **TelegramBot** получает сообщение от пользователя
2. **ConversationManager** добавляет сообщение в контекст диалога
3. **ConversationManager** формирует промпт для LLM
4. **LLMClient** отправляет запрос в OpenRouter
5. **LLMClient** возвращает ответ от LLM
6. **ConversationManager** обновляет контекст диалога
7. **TelegramBot** отправляет ответ пользователю

### Зависимости между классами
- **TelegramBot** → **ConversationManager** → **LLMClient**
- **Все классы** → **Config**
- Никаких циклических зависимостей

### Принципы взаимодействия
- Слабая связанность - классы знают только о том, что им нужно
- Простые интерфейсы - минимум методов
- Прямолинейный поток данных
- Обработка ошибок на уровне main.py

## Модель данных

### Простые структуры данных для MVP

#### 1. Сообщение пользователя
```python
# В conversation_manager.py
@dataclass
class UserMessage:
    user_id: int
    text: str
    timestamp: datetime
```

#### 2. Ответ LLM
```python
# В conversation_manager.py  
@dataclass
class LLMResponse:
    content: str
    timestamp: datetime
    model_used: str
```

#### 3. Контекст диалога
```python
# В conversation_manager.py
@dataclass
class ConversationContext:
    user_id: int
    messages: List[UserMessage]
    responses: List[LLMResponse]
    system_prompt: str
```

### Принципы хранения данных
- **В памяти** - для MVP не нужна база данных
- **Один диалог на пользователя** - простейшая реализация
- **Ограничение истории** - максимум 10 последних сообщений для экономии токенов
- **Автоочистка** - старые диалоги удаляются при перезапуске

### Структура данных
```
ConversationContext
├── user_id: int
├── system_prompt: str
├── messages: [UserMessage1, UserMessage2, ...]
└── responses: [LLMResponse1, LLMResponse2, ...]
```

### Ограничения для MVP
- Максимум 10 сообщений в истории диалога
- Только текстовые сообщения (без файлов, фото, стикеров)
- Один активный диалог на пользователя
- Нет персистентности - данные теряются при перезапуске

## Работа с LLM

### Интеграция через OpenRouter

#### Основные параметры
- **Провайдер**: OpenRouter API
- **Клиент**: официальная библиотека `openai`
- **Модель по умолчанию**: `openai/gpt-3.5-turbo` (баланс цена/качество)
- **Метод**: `chat.completions.create()`

#### Структура запроса
```python
# В llm_client.py
{
    "model": "openai/gpt-3.5-turbo",
    "messages": [
        {"role": "system", "content": "system_prompt"},
        {"role": "user", "content": "user_message_1"},
        {"role": "assistant", "content": "assistant_response_1"},
        {"role": "user", "content": "current_message"}
    ],
    "max_tokens": 1000,
    "temperature": 0.7
}
```

#### Обработка ответа
- Извлечение `content` из `choices[0].message.content`
- Простая обработка ошибок (retry, fallback)
- Логирование запросов и ответов

#### Конфигурация
```python
# В config.py
OPENROUTER_API_KEY = "your_key"
DEFAULT_MODEL = "openai/gpt-3.5-turbo"
MAX_TOKENS = 1000
TEMPERATURE = 0.7
```

### Принципы работы
- **Простота** - минимум настроек, максимум по умолчанию
- **Надежность** - обработка ошибок сети и API
- **Экономия** - ограничение токенов и истории
- **Прозрачность** - логирование для отладки

## Сценарии работы

### Основной сценарий (Happy Path)
1. **Запуск бота** - `python src/main.py`
2. **Пользователь отправляет `/start`** - бот приветствует и объясняет возможности
3. **Пользователь пишет сообщение** - бот обрабатывает и отвечает через LLM
4. **Диалог продолжается** - контекст сохраняется в памяти
5. **Перезапуск бота** - контекст теряется, диалог начинается заново

### Дополнительные сценарии

#### Обработка ошибок
- **Ошибка LLM API** - бот отвечает "Извините, произошла ошибка. Попробуйте позже"
- **Пустое сообщение** - бот игнорирует
- **Слишком длинное сообщение** - бот отвечает "Сообщение слишком длинное"

#### Команды бота
- **`/start`** - приветствие и инструкции
- **`/help`** - справка по командам
- **`/clear`** - очистка истории диалога
- **Любое другое сообщение** - отправка в LLM

### Пользовательский опыт
- **Простота** - никаких сложных команд
- **Отзывчивость** - быстрые ответы (в пределах API)
- **Понятность** - четкие сообщения об ошибках
- **Интуитивность** - работает как обычный чат

### Ограничения MVP
- Только текстовые сообщения
- Один диалог на пользователя
- Нет сохранения между перезапусками
- Нет настройки роли через команды

## Подход к конфигурированию

### Простая конфигурация через .env

#### Структура .env файла
```bash
# Telegram Bot
TELEGRAM_BOT_TOKEN=your_telegram_bot_token

# OpenRouter API
OPENROUTER_API_KEY=your_openrouter_api_key

# LLM Settings
DEFAULT_MODEL=openai/gpt-3.5-turbo
MAX_TOKENS=1000
TEMPERATURE=0.7

# Bot Settings
MAX_HISTORY_MESSAGES=10
SYSTEM_PROMPT=You are a helpful AI assistant.

# Logging
LOG_LEVEL=INFO
```

#### Загрузка конфигурации
```python
# В config.py
class Config:
    def __init__(self):
        load_dotenv()
        self.telegram_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.openrouter_key = os.getenv('OPENROUTER_API_KEY')
        self.default_model = os.getenv('DEFAULT_MODEL', 'openai/gpt-3.5-turbo')
        # ... остальные параметры
```

### Принципы конфигурации
- **Все в .env** - никаких config файлов
- **Значения по умолчанию** - для необязательных параметров
- **Валидация при запуске** - проверка обязательных токенов
- **Простота** - минимум настроек, максимум работает "из коробки"

### Файлы конфигурации
- **`.env`** - реальные настройки (в .gitignore)
- **`.env.example`** - шаблон для разработчиков
- **`config.py`** - класс для загрузки и валидации

### Валидация
- Проверка наличия обязательных токенов при запуске
- Простые сообщения об ошибках
- Остановка бота при отсутствии критических настроек

## Подход к логгированию

### Простое консольное логирование

#### Настройка логирования
```python
# В main.py
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
```

#### Уровни логирования
- **INFO** - основная информация (запуск, получение сообщений, ответы)
- **ERROR** - ошибки (проблемы с API, исключения)
- **DEBUG** - детальная отладка (только при разработке)

#### Что логируем
- **Запуск/остановка бота**
- **Получение сообщений от пользователей** (user_id, текст)
- **Запросы к LLM** (модель, количество токенов)
- **Ответы от LLM** (время обработки, длина ответа)
- **Ошибки** (тип ошибки, детали)

#### Структура логов
```
2024-01-15 10:30:15 - TelegramBot - INFO - Bot started
2024-01-15 10:30:20 - TelegramBot - INFO - Received message from user 123: "Привет"
2024-01-15 10:30:21 - LLMClient - INFO - Sent request to openai/gpt-3.5-turbo
2024-01-15 10:30:22 - LLMClient - INFO - Received response (150 tokens, 1.2s)
2024-01-15 10:30:22 - TelegramBot - INFO - Sent response to user 123
```

### Принципы логирования
- **Простота** - только консоль, никаких файлов
- **Информативность** - достаточно для отладки
- **Производительность** - минимум влияния на скорость
- **Безопасность** - не логируем токены и личные данные

### Конфигурация через .env
```bash
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
```

---

**Документ создан:** 2024-01-15  
**Версия:** 1.0  
**Статус:** Готов к реализации
